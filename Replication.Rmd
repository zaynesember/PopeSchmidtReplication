---
title: | 
  | Replication of Father Founders: 
  | Did Child Gender Affect Voting at the Constitutional Convention?
subtitle: |
  | Prepared for POLI 271: Advanced Statistical Applications
author: |
  | Zayne Sember
  | Department of Political Science
  | University of California, San Diego
abstract: | 
  | This article is a replication of Jeremy C. Pope and Soren J. Schmidt's 2021 piece "Father Founders: Did Child Gender Affect Voting at the Constitutional Convention?". In it, they test the hypothesis that the delegates with sons would tend to vote for a stronger national government because they foresaw such a government providing greater opportunities for their sons--for which they find evidence. I begin by replicating their primary model, a Poisson regression on a vote index, and their probit models of individual votes. I then examine the distributions of underlying data to ensure their use of Poisson regression is appropriate. I next assess the differences in standard errors for their model covariates comparing their `STATA` robust standard errors with `R`'s `glm()` standard errors and bootstrapped standard errors. Finally, I use the dataset from which the paper's dataset was derived to assess missingness and compare imputation to the historical supplementation method used by the authors. 
output: pdf_document
header-includes:
   - \usepackage[default]{sourcesanspro}
   - \usepackage[T1]{fontenc}
   - \usepackage[utf8]{inputenc}
   - \usepackage{setspace}\doublespacing
mainfont: SourceSansPro
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE)
library(haven)
library(ggplot2)
library(tidyverse)
library(sandwich)
library(grid)
library(gridExtra)
library(corrplot)
library(Amelia)
library(ggeffects)
library(modelsummary)
library(AER)
library(sampleSelection)
library(stargazer)
library(kableExtra)
library(vcd)
library(MASS)
library(COUNT)
library(boot)
library(Amelia)
library(reshape2)
```

```{r, message=FALSE}
# Get the replication dataset
dataset <- read_dta("Original Dataset/founding fathers dataset-corrections-attendance-replication.dta")

# Get the ICPSR delegate vote dataset
ICPSR_data <- read_delim("ICPSR Dataset/ICPSR24544-Data.tsv", 
                         "\t", escape_double = FALSE, trim_ws = TRUE) %>% 
              dplyr::select(firstname, lastname, state, state_num,
                            dv30, dv230, dv268, dv336, dv345,
                            dv387, dv399, dv415)

# Recode missing data
ICPSR_data[,5:12] <- na_if(ICPSR_data[,5:12], 9)
ICPSR_data[,5:12] <- na_if(ICPSR_data[,5:12], 0)
ICPSR_data[,5:12][ICPSR_data[,5:12]==6] <- 0

# Recode anti votes
ICPSR_data[,c(6,7,8,11)][ICPSR_data[,c(6,7,8,11)]==1] <- 0
ICPSR_data[,c(6,7,8,11)][ICPSR_data[,c(6,7,8,11)]==6] <- 1

# Recode vote votes
ICPSR_data[,c(5,9,10,12)][ICPSR_data[,c(5,9,10,12)]==6] <- 0

colnames(ICPSR_data) <- c("firstname", "lastname", "state", "statenum", "vote2",
                          "anti5", "anti6", "anti7", "vote8","vote9", "anti14",
                          "vote15")
```

## Data

The replication data used is provided by Pope and Schmidt (2021). I also employ Dougherty and Heckelman's (2009) dataset which provides delegate vote data without the historical supplementation of votes present in Pope and Schmidt's replication data. All code and data for this project is publicly available at https://github.com/zaynesember/PopeSchmidtReplication.

&nbsp;  
&nbsp;  
&nbsp;  
\begin{center}
\textbf{Figure 1: Correlation Matrix of Variables}
\end{center}

```{r, out.width=1000, fig.show="hold"}
n <- dataset %>% dplyr::select(index1, vote2, anti5, anti6, anti7, vote8, vote9, anti14, vote15, sons, dtrs, ageco, agecosq, revoffco,
                     nslave, dist2, vsecr, vbank, ddebt, pols, lawyer)
m <- cor(n)

corrplot(m, method="color", addCoef.col = "black",
         tl.col="gray16", tl.srt=45, number.cex=0.4,
         title="")
```

## Results

### Model Replication

Pope and Schmidt (2021) present 9 models of interest, all of which share the same covariates (with the exception of a covariate being omitted in two of their probit models due to what the authors deem to be multicollinearity but may be better termed perfect separation). Their primary model is a Poisson regression on a "preferred index" of the eight votes evaluated in each of the additional probit models. The preferred index is calculated by summing the number of "yea" votes for expanding the national government and "nay" votes on limiting the government across the eight key votes examined in their probit models. Appendix A provides the table of all independent and dependent variables. Dependent variables labeled "vote" indicate a vote where a "yea" expands the government and those labeled "anti" denote a vote limiting government. A correlation matrix of all variables is shown in Figure 1. Many of the individual votes (`vote2` through `vote15`) are correlated with one another but this does not threaten any assumptions of the models. Of the independent variables, none are unexpectedly strongly correlated with each other or with the dependent variables.

Table 1 presents the replication of these nine models. All coefficients agree with those presented by Pope and Schmidt, although some standard errors in the probit models differ very slightly. This is due to the original analysis being done in `STATA` with the `robust` command; the similar robust errors reported here were calculated in `R` using the `sandwich` library's `vcovCL` function. The National Exports and State Credit models omit three and twelve observations, respectively. These are omitted to replicate the manner in which `STATA` handles probit models with perfect separation. The debtor and private securities coefficients are omitted from these models due to perfect multicollinearity after the observations are dropped.

```{r}
model_poisson_R <- glm(index1 ~ sons + dtrs + ageco + agecosq + revoffco
                     + nslave + dist2 + vsecr + vbank + ddebt + pols + lawyer,
                     data=dataset, family = poisson(link = "log"))

# Generate predicted values for significant coefficients
pred1 <- ggpredict(model_poisson_R, c("sons"))


# Plot the predicted values
pred_sons <- ggplot(pred1, aes(x=x,y=predicted)) +
  geom_line() +
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = .25) +
  xlab("Number of sons") +
  ylab("Predicted pro-national votes") +
  ylim(c(0,10)) +
  xlim(c(0,3)) +
  theme_bw()

# Rinse and repeat
pred2 <- ggpredict(model_poisson_R, c("dist2"))



pred_dist <- ggplot(pred2, aes(x=x,y=predicted)) +
  geom_line() +
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = .25) +
  xlab("Distance from coastline") +
  ylab("Predicted pro-national votes") +
  ylim(c(0,10)) +
  theme_bw()


pred3 <- ggpredict(model_poisson_R, c("nslave"))


pred_nslave <- ggplot(pred3, aes(x=x,y=predicted)) +
  geom_line() +
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = .25) +
  xlab("Number of slaves") +
  ylab("Predicted pro-national votes") +
  ylim(c(0,10)) +
  theme_bw()

grid.arrange(pred_sons, pred_dist, pred_nslave, nrow=1, top="Figure 2: Predicted Value Plots")
```

Figure 2 replicates Pope and Schmidt's (2021) Figure 2, providing predicted value plots for the variable of interest (number of sons) and two other influential variables. As they conclude, there is clear evidence that delegates with more sons tended to vote more in favor of expanding the national government at a magnitude comparable to other pertinent variables such as distance from a navigable coastline and number of slaves owned. 

```{r, warning=FALSE}
# Code taken from:
# https://stats.stackexchange.com/questions/89999/
# how-to-replicate-statas-robust-binomial-glm-for-proportion-data-in-r
# Creates robust standard errors for glm models
# NO LONGER NEEDED/USED
robustify <- function(model){
  cov.m1 <- vcovHC(model, type = "HC0")
  
  std.err <- sqrt(diag(cov.m1))
  
  q.val <- qnorm(0.975)
  
  retVal <- cbind(
    Estimate = coef(model)
    , "Robust SE" = std.err
    , z = (coef(model)/std.err)
    , "Pr(>|z|) "= 2 * pnorm(abs(coef(model)/std.err), 
                             lower.tail = FALSE)
    , LL = coef(model) - q.val  * std.err
    , UL = coef(model) + q.val  * std.err
  )
  return(retVal)
}

# Because R and STATA don't get along, this function takes a 
# sampleSelection::probit model with robust SEs and formats the output to be
# presented in a neat table since stargazer and modelsummary don't support
# the model output by sampleSelection
# NO LONGER NEEDED/USED
fmt_col <- function(model, DV){
  ests <- rep(NA, nrow(model))
  errs <- rep(NA, nrow(model))
  
  for(i in 1:nrow(model)){
    sig <- ""
    if(model$p[i] < 0.01){sig <- '***'}
    else if(model$p[i] < 0.05){sig <- '**'}
    else if(model$p[i] < 0.1){sig <- '*'}
    
    ests[i] <- paste(model$estimate[i],sig, sep="")
    errs[i] <- paste("(",model$stderr[i],")",sep="")
  }
  return(c(rbind(ests,errs),toString(length(DV))))
}

# Replicate each model
model_poisson_R <- glm(index1 ~ sons + dtrs + ageco + agecosq + revoffco
                     + nslave + dist2 + vsecr + vbank + ddebt + pols + lawyer,
                     data=dataset, family = poisson(link = "log"))

model_probit1_R <- glm(vote2 ~ sons + dtrs + ageco + agecosq +
                    revoffco + nslave + dist2 + vsecr + vbank +
                    ddebt + pols + lawyer,
                    data=dataset, family=binomial(link="probit"))

model_probit2_R <- glm(anti5 ~ sons + dtrs + ageco + agecosq +
                    revoffco + nslave + dist2 + vsecr + vbank +
                    ddebt + pols + lawyer,
                    data=dataset, family=binomial(link="probit"))

model_probit3_R <- glm(anti6 ~ sons + dtrs + ageco + agecosq +
                    revoffco + nslave + dist2 + vsecr + vbank +
                    ddebt + pols + lawyer,
                    data=dataset, family=binomial(link="probit"))

# Remove observations dropped by STATA
temp <- dataset[-c(3,12,19),]
model_probit4_R <- glm(anti7 ~ sons + dtrs + ageco + agecosq +
                    revoffco + nslave + dist2 + vsecr + vbank +
                    ddebt + pols + lawyer,
                    data=temp, family=binomial(link="probit"))

model_probit5_R <- glm(vote8 ~ sons + dtrs + ageco + agecosq +
                    revoffco + nslave + dist2 + vsecr + vbank +
                    ddebt + pols + lawyer,
                    data=dataset, family=binomial(link="probit"))

# Remove observations dropped by STATA
temp <- dataset[-c(9,15,28,33,36,37,40:43,50,51),]
model_probit6_R <- glm(vote9 ~ sons + dtrs + ageco + agecosq +
                    revoffco + nslave + dist2 + vsecr + vbank +
                    ddebt + pols + lawyer,
                    data=temp, family=binomial(link="probit"))

model_probit7_R <- glm(anti14 ~ sons + dtrs + ageco + agecosq +
                    revoffco + nslave + dist2 + vsecr + vbank +
                    ddebt + pols + lawyer,
                    data=dataset, family=binomial(link="probit"))

model_probit8_R <- glm(vote15 ~ sons + dtrs + ageco + agecosq +
                    revoffco + nslave + dist2 + vsecr + vbank +
                    ddebt + pols + lawyer,
                    data=dataset, family=binomial(link="probit"))

# Store models and their standard errors, output into a table
model_list <- list('Preferred Index' = model_poisson_R, 
                   'National Veto'=model_probit1_R, 
                   'Debtor Legislators'=model_probit2_R,
                   'Cong. Quorum'=model_probit3_R, 
                   'National Exports'=model_probit4_R, 
                   'Militia Control'=model_probit5_R,
                   'State Credit'=model_probit6_R, 
                   'Navigation Acts'=model_probit7_R, 
                   'Military Responsibility'=model_probit8_R)

SE_list <- list(coeftest(model_list[[1]], vcovCL)[,2],
                coeftest(model_list[[2]], vcovCL)[,2],
                coeftest(model_list[[3]], vcovCL)[,2],
                coeftest(model_list[[4]], vcovCL)[,2],
                coeftest(model_list[[5]], vcovCL)[,2],
                coeftest(model_list[[6]], vcovCL)[,2],
                coeftest(model_list[[7]], vcovCL)[,2],
                coeftest(model_list[[8]], vcovCL)[,2],
                coeftest(model_list[[9]], vcovCL)[,2]
                )

cnames <- c("Preferred Index","National Veto","Debtor Legislators",
            "Cong. Quorum.","National Exports","Militia Control",
            "State Credit","Navigation Acts","Military Responsibility")

rnames <- c('(Intercept)'="Constant",
            'sons'="Number of sons",
            'dtrs'="Number of daughters",
            'ageco'="Age",
            'agecosq'="Age squared",
            'revoffco'="Revolutionary war officer",
            'nslave'="Logged number of slaves",
            'dist2'="Distance to navigable coastline",
            'vsecr'="Public securities (1000s, 1787 dollars)",
            'vbank'="Private securities (1000s, 1787 dollars)",
            'ddebt'="Debtor (dummy)",
            'pols'="Politician",
            'lawyer'="Lawyer")


table2 <- modelsummary(model_list, statistic_override=SE_list, stars=T,
                       coef_rename=rnames,
                       notes=c("NA coefficients omitted due to perfect
                               multicollinearity.",
                               "Preferred index model is a Poisson regression,
                               all others are probit.
                               Standard errors are robust."),
                       title="Model Replications",
                       output="kableExtra")

table2 %>% kable_styling(latex_options="scale_down") %>% landscape

```

```{r, message=FALSE, warning=FALSE, eval=FALSE}
# Plot the individual vote distributions
vote1 <- ggplot(aes(x=vote2),data=dataset) +
  geom_histogram(binwidth = 0.5, col="darkgray", fill="coral4") +
  xlab("National Veto Vote") +
  ylab("") +
  scale_x_continuous(breaks = c(0,1)) +
  theme_bw()

vote2 <- ggplot(aes(x=anti5),data=dataset) +
  geom_histogram(binwidth = 0.5, col="darkgray", fill="chocolate2") +
  xlab("Debtor Legislators Vote") +
  ylab("") +
  scale_x_continuous(breaks = c(0,1)) +
  theme_bw()

vote3 <- ggplot(aes(x=anti6),data=dataset) +
  geom_histogram(binwidth = 0.5, col="darkgray", fill="cadetblue3") +
  xlab("Cong. Quorum Vote") +
  ylab("") +
  scale_x_continuous(breaks = c(0,1)) +
  theme_bw()

vote4 <- ggplot(aes(x=anti7),data=dataset) +
  geom_histogram(binwidth = 0.5, col="darkgray", fill="deeppink3") +
  xlab("National Exports Vote") +
  ylab("") +
  scale_x_continuous(breaks = c(0,1)) +
  theme_bw()

vote5 <- ggplot(aes(x=vote8),data=dataset) +
  geom_histogram(binwidth = 0.5, col="darkgray", fill="darkorchid3") +
  xlab("Militia Control Vote") +
  ylab("") +
  scale_x_continuous(breaks = c(0,1)) +
  theme_bw()

vote6 <- ggplot(aes(x=vote9),data=dataset) +
  geom_histogram(binwidth = 0.5, col="darkgray", fill="indianred3") +
  xlab("State Credit Vote") +
  ylab("") +
  scale_x_continuous(breaks = c(0,1)) +
  theme_bw()

vote7 <- ggplot(aes(x=anti14),data=dataset) +
  geom_histogram(binwidth = 0.5, col="darkgray", fill="seagreen3") +
  xlab("Navigation Acts Vote") +
  ylab("") +
  scale_x_continuous(breaks = c(0,1)) +
  theme_bw()

vote8 <- ggplot(aes(x=vote15),data=dataset) +
  geom_histogram(binwidth = 0.5, col="darkgray", fill="thistle4") +
  xlab("Military Responsibility Vote") +
  ylab("") +
  scale_x_continuous(breaks = c(0,1)) +
  theme_bw()

grid.arrange(vote1, vote2, vote3, vote4, vote5, vote6, vote7, vote8,
             ncol=3, nrow=3, top="Figure 1: Individual Vote Distributions", widths=c(0.25,0.25,0.25))
```

```{r, eval=FALSE}
# Checking whether mean and variance of index1 are equal as Poisson regression
# assumes
m <- mean(dataset$index1)
cat("Mean:",m,"\n")
v <- var(dataset$index1)
cat("Variance:",v,"\n")

numstd <- abs(m-v)/sqrt(v)
cat("Num. of stddev:",numstd)

# Run an auxilliary OLS test for equidispersion
dispersiontest(model_poisson_R, alternative="less")
```

```{r, warning=FALSE}
# Function stolen from the vcd library. Provides just the data needed for a 
# Poissonness plot without actually plotting it which lets you use ggplot or 
# any graphics library you want!
distplot_hacked <-
    function(x, type = c("poisson", "binomial", "nbinomial"),
             size = NULL, lambda = NULL, legend = TRUE, xlim = NULL, ylim = NULL,
             conf_int = TRUE, conf_level = 0.95, main = NULL,
             xlab = "Number of occurrences", ylab = "Distribution metameter",
             gp = gpar(cex = 0.8), lwd=2, gp_conf_int = gpar(lty = 2),
             name = "distplot", newpage = TRUE,
             pop = TRUE, return_grob = FALSE, ...)
{
    if(is.vector(x)) {
        x <- table(x)
    }
    if(is.table(x)) {
        if(length(dim(x)) > 1) stop ("x must be a 1-way table")
        freq <- as.vector(x)
        count <- as.numeric(names(x))
    } else {
        if(!(!is.null(ncol(x)) && ncol(x) == 2))
            stop("x must be a 2-column matrix or data.frame")
        freq <- as.vector(x[,1])
        count <- as.vector(x[,2])
    }

    myindex <- (1:length(freq))[freq > 0]
    mycount <- count[myindex]
    myfreq <- freq[myindex]

    switch(match.arg(type),

           "poisson" = {
               par.ml <- suppressWarnings(goodfit(x, type = type)$par$lambda)

               phi <- function(nk, k, N, size = NULL)
                   ifelse(nk > 0, lgamma(k + 1) + log(nk/N), NA)
               y <- phi(myfreq, mycount, sum(freq))
               if(!is.null(lambda)) y <- y + lambda - mycount * log(lambda)
               fm <- lm(y ~ mycount)
               par.estim <- exp(coef(fm)[2])
               names(par.estim) <- "lambda"
               txt <- "exp(slope)"
               if(!is.null(lambda)) {
                   par.estim <- par.estim * lambda
                   txt <- paste(txt, "x lambda")
               }
               legend.text <- paste(txt, "=", round(par.estim, digits = 3))
               if(is.null(main)) main <- "Poissoness plot"
  },

           "binomial" = {
               if(is.null(size)) {
                   size <- max(count)
                   warning("size was not given, taken as maximum count")
               }
               par.ml <- suppressWarnings(goodfit(x, type = type, par = list(size = size))$par$prob)

               phi <- function(nk, k, N, size = NULL)
                   log(nk) - log(N * choose(size, k))
               y <- phi(myfreq, mycount, sum(freq), size = size)
               fm <- lm(y ~ mycount)
               par.estim <- exp(coef(fm)[2])
               par.estim <- par.estim / (1 + par.estim)
               names(par.estim) <- "prob"
               legend.text <- paste("inv.logit(slope) =", round(par.estim, digits = 3))
               if(is.null(main)) main <- "Binomialness plot"
           },

           "nbinomial" = {
               if(is.null(size)) {
                   par.ml <- suppressWarnings(goodfit(x, type = type)$par)
                   size <- par.ml$size
                   par.ml <- par.ml$prob
               }else{
                   xbar <- weighted.mean(mycount, myfreq)
                   par.ml <- size / (size+xbar)
               }
               phi <- function(nk, k, N, size = NULL)
                   log(nk) - log(N * choose(size + k - 1, k))
               y <- phi(myfreq, mycount, sum(freq), size = size)
               fm <- lm(y ~ mycount)
               par.estim <- 1 - exp(coef(fm)[2])
               names(par.estim) <- "prob"
               legend.text <- paste("1-exp(slope) =", round(par.estim, digits = 3))
               if(is.null(main)) main <- "Negative binomialness plot"
           })

    yhat <- ifelse(myfreq > 1.5, myfreq - 0.67, 1/exp(1))
    yhat <- phi(yhat, mycount, sum(freq), size = size)
    if(!is.null(lambda)) yhat <- yhat + lambda - mycount * log(lambda)

    phat <- myfreq / sum(myfreq)
    ci.width <- qnorm(1-(1 - conf_level)/2) *
        sqrt(1-phat)/sqrt(myfreq - (0.25 * phat + 0.47)*sqrt(myfreq))

    RVAL <- cbind(count, freq, NA, NA, NA, NA, NA)
    RVAL[myindex,3:7] <- cbind(y,yhat,ci.width, yhat-ci.width, yhat + ci.width)
    RVAL <- as.data.frame(RVAL)
    names(RVAL) <- c("Counts", "Freq", "Metameter", "CI.center",
                     "CI.width", "CI.lower", "CI.upper")
    

    x_lim <- range(RVAL[,1])
    y_line <- predict(fm, newdata = data.frame(mycount = xlim))
    
    # Inserted return statement to give just what's needed
    RVAL$y_line <- y_line
    return(RVAL)

  }
```


```{r, warning=FALSE, out.height="75%", out.width="75%", fig.align="center"}
# Old function call
#distplot(as.vector(dataset$index1), type="poisson")

# New function call, takes the same arguments but returns a dataframe with
# everything needed to make a Poissonness plot
RVAL <- distplot_hacked(as.vector(dataset$index1), type="poisson")

ggplot(RVAL,aes()) + 
  geom_line(aes(x=Counts,y=y_line, color="Perfect Poisson distribution"), 
            size=0.75, key_glyph="point") +
  geom_point(aes(x=Counts, y=Metameter, color="Observed distribution"),
             key_glyph="point") +
  xlab("Count") +
  ylab("Distribution metameter") +
  ggtitle("Figure 3: Poissoness Plot of Preferred Index") +
  labs(color="", position="bottom") +
  scale_colour_manual(values=c("red", "blue")) +
  theme_bw() +
  theme(legend.position="top") +
  annotate(geom="text",x=1,y=7,label="slope = 1.708\nintercept = -5.441\nlambda = 5.17\nexp(slope) = 5.516",
           hjust=0) +
  scale_x_continuous(breaks = 1:8) +
  scale_y_continuous(breaks = -4:9)

```

### Distribution of Data

I next examine the distribution of the primary dependent variable: the preferred index of individual votes. In order for a Poisson regression on this index to be valid, it needs to follow a Poisson distribution with the key assumption that the count data's mean is equal to its variance. A simple check of this on the preferred index yields a mean of 5.17 and variance of 5.34, which falls 0.07 standard deviations away--a very minor violation of this strict assumption. Figure 3 provides a more visual test of the distribution with a Poissoness plot. If the data were perfectly Poisson then we would expect the intercept of the Poissoness plot to be $-\lambda=-5.17$; in reality it is $-5.441$. We would also expect the slope to be $\log(\lambda)=0.742$ whereas it is $1.708$ on the plot. Visual inspection of the plot shows the data does generally follow a Poisson distribution with the largest outlier being the number of observations where the preferred index equals one. Figures 4 and 5 compare the fit of the data to the Poisson and negative binomial distributions with rootograms. The data appears to fit both distributions well in this metric, again with an excess of ones being the primary departure from the expected distribution.

```{r, message=FALSE, eval=FALSE}
# Makes hanging rootograms
poisson_fit <- goodfit(table(dataset$index1), "poisson")
negbin_fit <- goodfit(table(dataset$index1), "nbinomial")


png(file="Figure4.png")
plot(poisson_fit, type = "hanging", scale = "sqrt", main="Figure 4: Poisson Rootogram")
dev.off()
png(file="Figure5.png")
plot(negbin_fit, type = "hanging", scale = "sqrt", main="Figure 5: Negative Binomial Rootogram")
dev.off()
```

```{r, out.width="49%", out.height="49%", fig.show='hold', fig.align='center'}
knitr::include_graphics(c("Figure4.png","Figure5.png"))
```

Table 2 shows a comparison of the preferred index model as a Poisson and negative binomial regression. The resulting coefficients and non-robust standard errors are identical. This is due to the fact that a Poisson regression is a special case of the negative binomial in which the variance and mean are equal. Because the mean and variance (as shown earlier) are only 0.07 standard deviations apart, the Poisson and negative binomial regressions are nearly indistinguishable in this case. This is further supported by their equal log likelihoods and nearly equal AIC and BICs and expected given the identical rootograms in Figures 4 and 5 which again highlight the excess of ones in the count data.

Taken in sum, the above evidence strongly indicates that Poisson regression is appropriate for modeling the preferred index. The count data follows an approximately Poisson distribution with nearly equal variance and mean, a lack of overdispersion, and produces identical results when modeled as a negative binomial distribution instead. The data is, of course, not perfectly Poisson, particularly when looking at the number of ones in the preferred index, but overall the assumptions of the Poisson regression are sufficiently met.

```{r, fig.show="hold"}
# Compare Poisson model with negative binomial
model_nb1 <- glm.nb(index1 ~ sons + dtrs + ageco + agecosq + revoffco
                     + nslave + dist2 + vsecr + vbank + ddebt + pols + lawyer,
                     data=dataset)

model_nb2 <- ml.nb2(index1 ~ sons + dtrs + ageco + agecosq + revoffco
                     + nslave + dist2 + vsecr + vbank + ddebt + pols + lawyer,
                     data=dataset)

IV_names <- list("(Intercept)"="Constant","sons"="Number of sons","dtrs"="Number of daughters","ageco"="Age",
         "agecosq"="Age squared","revoffco"="Revolutionary war officer","nslave"="Logged number of slaves",
         "dist2"="Distance to navigable coastline",
         "vsecr"="Public securities (1000s, 1787 dollars)",
         "vbank"="Private securities (1000s, 1787 dollars)",
         "ddebt"="Debtor (dummy)","pols"="Politician","lawyer"="Lawyer") 

modelsummary(list("Poisson"=model_poisson_R, "Negative Binomial"=model_nb1), stars=T,
             coef_map=IV_names, fmt=4, caption="Comparison of Poisson and Negative Binomial Models of the Preferred Index")

```

```{r, fig.show="hold", fig.align="center"}
# Function needed to generate bootstrapped standard errors for Poisson model
boot.poisson <- function(data, indices, maxit=20){
  data <- data[indices,]
  model <- glm(index1 ~ sons + dtrs + ageco + agecosq + revoffco
                     + nslave + dist2 + vsecr + vbank + ddebt + pols + lawyer,
                     data=data, family = poisson(link = "log"))
  #return(sqrt(diag(vcov(model))))
  return(model$coefficients)
}

# Function needed to generate bootstrapped standard errors for probit model
# NOT USED
boot.probit <- function(data, indices, formula, maxit=20){
  data <- data[indices,]
  model <- glm(formula, data=data, family=binomial(link="probit"))
  #return(sqrt(diag(vcov(model))))
  return(model$coefficients)
}

poisson_boot <- boot(dataset, boot.poisson, 1999, maxit=100)

# Store the three types of standard errors for later comparison
SE_poisson_R <- summary(model_poisson_R)$coefficients[,2]
SE_poisson_robust <- robustify(model_list[[1]])[,2]
SE_poisson_boot <- summary(poisson_boot)$bootSE
```

```{r, fig.align="center", out.width="75%", out.height="75%"}
# Makes a bar plot comparing standard error types
var_names <- c("Constant","Number of sons","Number of daughters","Age",
         "Age squared","Revolutionary war officer","Logged number of slaves",
         "Distance to navigable coastline",
         "Public securities (1000s, 1787 dollars)",
         "Private securities (1000s, 1787 dollars)",
         "Debtor (dummy)","Politician","Lawyer")

SEs <- data.frame(round(cbind(SE_poisson_R, SE_poisson_robust, SE_poisson_boot),3))
colnames(SEs) <- c("R glm", "Robust", "Bootstrapped")
rownames(SEs) <- var_names

xlabs_bar <- names(rnames)
xlabs_bar[[1]] <- "intercept"

SEs$coeff <- xlabs_bar

ggplot(data=melt(SEs),aes(x=coeff, y=value, fill=variable)) +
  geom_bar(stat="identity", position=position_dodge()) +
  xlab("Coefficient") +
  ylab("Estimated standard error") +
  ggtitle("Figure 6: Comparison of Preferred Index Model\nStandard Error Estimates") +
  labs(fill='Estimation method') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = -45))
```

```{r, fig.show="hold", fig.align="center"}
kable(SEs[,1:3], booktabs=T, caption ="Comparison of Preferred Index Model Standard Error Estimates") %>%
  kable_styling(position = "center")
```

### Standard Errors and Underdispersion

Table 3 reports the standard error estimates for coefficients in the preferred index model calculated through three different methods. The first are the errors as reported by `R`'s `glm` function, calculated by inverting the model's Fisher information matrix and taking the square root of its diagonals. Robust standard errors are the same as those reported in Table 1, calculated in an attempt to match the `STATA` robust standard errors reported by Pope and Schmidt (2021). Bootstrapped standard errors are calculated using the `boot` package in `R`. Evident from Figure 6's bar plot comparing these values, the robust errors are consistently the smallest. The `glm` and bootstrapped standard errors tend to be of similar magnitude (with the exception of the intercept). 

When robust standard errors are smaller than non-robust errors as in this case, it can indicate underdispersion of the count data which inflates non-robust standard errors. Testing the alternative hypothesis that the data is underdispersed (and thus its dispersion parameter is less than one) against the null hypothesis that it is equidispersed (its dispersion parameter is one) as assumed by the Poisson regression yields a statistically significant estimate at the $p<0.05$ level of the dispersion parameter being $0.71$ indicating underdispersion and the possibility that the non-robust errors are inflated.

On average, the non-robust errors in Table 3 are 47% larger (omitting age squared). Changing from robust to `glm` errors (or the nearly equivalent bootstrapped error estimate), the error estimate on the number of sons coefficient grows by 39% and its significance level changes from the $p<0.01$ reported by Pope and Schmidt (2021) to $p<0.05$. Because it remains significant and its coefficient estimate of 0.147 remains an order of magnitude larger than its estimated errors, the type of standard error estimates used does not change the substantive interpretation of that particular covariate's impact on delegates' vote choice. For this data, Pope and Schmidt's choice of robust standard errors seems justified as it avoids the inflation of the non-robust errors and does not change the interpretation of the coefficient of interest.

### Counterfactual Delegates

Inspection of delegate vote data from Dougherty and Heckelman (2009) reveals two delegates not included by Pope and Schmidt (2021). The first is George Wythe of Virginia's delegation who had no children and left the Convention in June to tend to his dying wife without casting any votes (Holt n.d.). It is not clear whether he was replaced but he was a noted Federalist and thus likely supported a strong national government. The second is William Churchill Houston of New Jersey who left two sons and three daughters when he died one week into the convention, having also cast no votes (Glenn 2013). He was replaced by William Livingston ("The Founding Fathers: New Jersey" 2015).

I use the latter of these delegates as an opportunity to both assess the sensitivity of the preferred index model and to consider a counterfactual Convention in which William Churchill Houston was able to vote. Because the preferred index is calculated by summing eight individual votes with two potential values, creating every possible vote scenario for a counterfactual delegates is computationally trivial^[Accounting for every combination of two delegates' votes, on the other hand, increases the number of scenarios from $2^8=256$ to $2^{16}=65,536$ scenarios]. Along with the variable of interest (number of sons), covariates that could be easily found were imputed (age, whether they were a lawyer, etc.), all others were held at the median of the other delegates. Since William Livingston would not have attended had Houston been present, I remove his votes from the counterfactual analysis.

```{r, eval=FALSE}
# Set all variables to their median value as baseline for Houston
Houston_vals <- summarize_all(dataset, median)
# Put in known biographical details
Houston_vals$state <- "NJ"
Houston_vals$delegates <- "Houston"
Houston_vals$sons <- 2
Houston_vals$dtrs <- 3
Houston_vals$total <- 5
Houston_vals$pctdtr <- 3/5
Houston_vals$pctson1 <- NA
Houston_vals$age <- 41
Houston_vals$ageco <- 41
Houston_vals$agecosq <- 41^2
Houston_vals$revoff <- 1
Houston_vals$revoffco <- 1
Houston_vals$lawyer <- 1
Houston_vals$dist2 <- 14 # Imputed floored mean of other NJ delegates

# Attach to the dataset
dataset_Houston <- rbind(dataset, Houston_vals) %>% filter(delegates!="Livingston")

# Get all the possible combinations of votes Houston could have had
possible_votes <- expand.grid(rep(list(0:1),8))
colnames(possible_votes) <-  c("vote2", "anti5", "anti6", "anti7", "vote8", 
                               "vote9", "anti14", "vote15")
possible_votes$index1 <- NA

# Calculate the index
for(i in 1:nrow(possible_votes)){
  possible_votes[i,9] <- sum(possible_votes[i,1:8])
}

```

```{r, eval=FALSE}
# Loop through every possible vote and get model and other stuff for every
# scenario
coeffs <- c()
errs <- c()
pvals <- c()
preds <- list()
for(i in 1:nrow(possible_votes)){
  
  data <- dataset_Houston
  
  data[53,c(49,65,66,67,55,56,68,62,74)] <- possible_votes[i,]
  
  mod <- glm(index1 ~ sons + dtrs + ageco + agecosq + revoffco
             + nslave + dist2 + vsecr + vbank + ddebt + pols + lawyer,
             data=data, family = poisson(link = "log"))
  preds <- append(preds, list(ggpredict(mod, c("sons"))))
  coeffs <- c(coeffs, summary(mod)$coefficients[2,1])
  errs <- c(errs, summary(mod)$coefficients[2,2])
  pvals <- c(pvals, coef(summary(mod))[,4][2])
}

# This plotting method stolen from 
# https://stackoverflow.com/questions/42229750/plot-lines-in-ggplot-from-a-list-of-dataframes/42230237
p <- ggplot()

plot <- function(df){
    p <<- p + geom_line(data=df, 
                        aes(x=x,y=predicted,col="darkgray"), 
                        size = 0.25, linetype="solid")
}

lapply(preds, plot)

p + geom_line(data=pred1, aes(x=x,y=predicted,col="red"),
              linetype="dashed", size=0.25) +
  xlab("Number of sons") +
  ylab("Predicted pro-national votes") +
  ggtitle("Figure 7: Counterfactual Scenario Predicted Value Plot") +
  scale_colour_manual(name = '', 
                      values =c('darkgray'='darkgray','red'='red'), 
                      labels = c('counterfactuals','actual'),
                      guide="legend") +
  ylim(c(0,8)) +
  theme_bw()
  
ggsave("counterfactplot.png")
```

```{r, fig.show='hold', fig.align='center', out.width="65%", out.height="65%"}
knitr::include_graphics("counterfactplot.png")
```

```{r, eval=FALSE}
# Plot the distribution of counterfactual coefficient estimates
fig8 <- ggplot(data.frame(coeffs),aes(x=coeffs)) +
  geom_histogram(color="darkgray", fill="darkblue", bins=10) +
  ggtitle("Figure 8: Counterfactual Number of Sons\nCoefficient Distribution") +
  xlab("Coefficient") +
  ylab("") +
  theme_bw() +
  theme(text = element_text(size=16))
ggsave("fig8.png")

# Plot the distribution of counterfactual coefficient p-values
fig9 <- ggplot(data.frame(pvals),aes(x=pvals)) +
  geom_histogram(color="darkgray", fill="darkred", bins=10) +
  ggtitle("Figure 9: Counterfactual Number of Sons\np-value Distribution") +
  xlab("p-value") +
  ylab("") +
  theme_bw() +
  theme(text = element_text(size=16))
ggsave("fig9.png")
```

```{r, fig.show='hold', fig.align='center', out.width="49%", out.height="49%"}
knitr::include_graphics(c("fig8.png", "fig9.png"))
```

Figure 7 shows the predicted value plot of the marginal effect of a delegate's number of sons on their predicted pro-national government votes for every counterfactual scenario (gray) and the observed scenario (red). Confidence intervals are omitted for ease of viewing. It is clear from this plot that the model's estimates would be virtually indistinguishable no matter how Houston voted. Figures 8 and 9 show the distribution of number of sons coefficient estimates and p-values for every counterfactual scenario. Since the coefficient distribution is centered around $0.148$ and the coefficient reported for the non-counterfactual model is $0.147$, most of the counterfactuals resulted in a very slight increase in this particular coefficient. The distribution of p-values reveals that the coefficient remains significant at the $p<0.05$ level (using non-robust standard errors) for every scenario. These results indicate that, in the alternative history where William Churchill Houston voted at the Convention rather than William Livingston, the number of sons delegates likely would have remained a strong predictor of whether they supported expansion of the national government regardless of how Houston voted.

```{r, fig.show="hold"}
DVs_missing <- ICPSR_data %>% dplyr::select(vote2, anti5, anti6, anti7, vote8,
                               vote9, anti14, vote15)

# Drop the last two delegate observations which Pope and Schmidt omit
DVs_missing <- DVs_missing[1:53,]

# Generate a missingness map
missmap(DVs_missing, main="Figure 10: Individual Vote Missingness")
```


```{r, eval=FALSE}
dataset_missing <- cbind(dataset %>% dplyr::select(sons, dtrs, ageco, 
                                                   agecosq, revoffco, nslave,
                                                   dist2, vsecr, vbank, ddebt,
                                                   pols, lawyer), 
                         DVs_missing)

# Have Amelia generate 5 imputed datasets for vote2
amelia.out <- amelia(x=dataset_missing %>% 
                       dplyr::select(-anti5, -anti6, -anti7, -vote8, 
                                     -vote9, -anti14, -vote15),
                     m=5, noms=c(5,10,11,12,13))

# Take the mean of the 5 imputed vote2 vectors
vote2_imp <- rowMeans(as.data.frame(amelia.out$imputations[[1]]$vote2,
                                    amelia.out$imputations[[2]]$vote2,
                                    amelia.out$imputations[[3]]$vote2,
                                    amelia.out$imputations[[4]]$vote2,
                                    amelia.out$imputations[[5]]$vote2))

# Rinse and repeat for vote9
amelia.out <- amelia(x=dataset_missing %>% 
                       dplyr::select(-vote2, -anti5, -anti6, -anti7, 
                                     -vote8, -anti14, -vote15),
                     m=5, noms=c(5,10,11,12,13))

vote9_imp <- rowMeans(as.data.frame(amelia.out$imputations[[1]]$vote9,
                                    amelia.out$imputations[[2]]$vote9,
                                    amelia.out$imputations[[3]]$vote9,
                                    amelia.out$imputations[[4]]$vote9,
                                    amelia.out$imputations[[5]]$vote9))

# Rinse and repeat for vote15
amelia.out <- amelia(x=dataset_missing %>% 
                       dplyr::select(-vote2, -anti5, -anti6, -anti7, 
                                     -vote8, -vote9, -anti14),
                     m=5, noms=c(5,10,11,12,13))

vote15_imp <- rowMeans(as.data.frame(amelia.out$imputations[[1]]$vote15,
                                    amelia.out$imputations[[2]]$vote15,
                                    amelia.out$imputations[[3]]$vote15,
                                    amelia.out$imputations[[4]]$vote15,
                                    amelia.out$imputations[[5]]$vote15))

dataset_missing <- dataset_missing %>% dplyr::select(-vote2, -anti5, -anti6, 
                                                     -anti7, -vote8, -vote9, 
                                                     -anti14, -vote15)

dataset_missing <- cbind(dataset_missing, vote2_imp, vote9_imp, vote15_imp)
```

```{r, eval=FALSE}
# Create the model with imputed data
model_probit_vote2_imp <- glm(vote2_imp ~ sons + dtrs + ageco + agecosq +
                              revoffco + nslave + dist2 + vsecr + vbank +
                              ddebt + pols + lawyer,
                              data=dataset_missing, 
                              family=binomial(link="probit"))
# Generate predicted values from imputed data
pred_vote2_imp <- ggpredict(model_probit_vote2_imp, c("sons"))
colnames(pred_vote2_imp) <- c("x.imp", "predicted.imp", "conf.low.imp",
                              "conf.high.imp", "group.imp")

# Generate predicted values from historically supplemented data
pred_vote2 <- ggpredict(model_probit1_R, c("sons"))

p2 <- cbind(pred_vote2_imp, pred_vote2)

# Plot the predicted values of both
pred_sons_vote2 <- ggplot(p2, aes()) +
  geom_line(aes(x=x,y=predicted, color="Historical supplement"), size=1) +
  #geom_ribbon(aes(x=x, y=predicted, ymin = conf.low, ymax = conf.high), alpha = .25) +
  geom_line(aes(x=x.imp, y=predicted.imp, color="Amelia imputed"), size=1) +
  #geom_ribbon(aes(x=x.imp, y=predicted.imp, ymin = conf.low.imp, ymax = conf.high.imp), alpha = .25) +
  xlab("Number of sons") +
  ylab("Probability of pro-national vote") +
  ylim(c(0,1)) +
  xlim(c(0,3)) +
  ggtitle("National Veto") +
  labs(color="")  +
  scale_color_manual(values = c("darkred", "darkblue")) +
  theme_bw() +
  theme(legend.position = "none",
        plot.title = element_text(size=12))

# Rinse and repeat
model_probit_vote9_imp <- glm(vote9_imp ~ sons + dtrs + ageco + agecosq +
                              revoffco + nslave + dist2 + vsecr + vbank +
                              ddebt + pols + lawyer,
                              data=dataset_missing, 
                              family=binomial(link="probit"))

pred_vote9_imp <- ggpredict(model_probit_vote9_imp, c("sons"))
colnames(pred_vote9_imp) <- c("x.imp", "predicted.imp", "conf.low.imp",
                              "conf.high.imp", "group.imp")

pred_vote9 <- ggpredict(model_probit6_R, c("sons"))

p9 <- cbind(pred_vote9_imp, pred_vote9)

pred_sons_vote9 <- ggplot(p9, aes()) +
  geom_line(aes(x=x,y=predicted, color="Historical supplement"), size=1) +
  #geom_ribbon(aes(x=x, y=predicted, ymin = conf.low, ymax = conf.high), alpha = .25) +
  geom_line(aes(x=x.imp, y=predicted.imp, color="Amelia imputed"), size=1) +
  #geom_ribbon(aes(x=x.imp, y=predicted.imp, ymin = conf.low.imp, ymax = conf.high.imp), alpha = .25) +
  xlab("Number of sons") +
  ylab("Probability of pro-national vote") +
  ylim(c(0,1)) +
  xlim(c(0,3)) +
  ggtitle("State Credit") +
  labs(color="")  +
  scale_color_manual(values = c("darkred", "darkblue")) +
  theme_bw() +
  theme(legend.position = c(0.5,0.2),
        plot.title = element_text(size=12))

# Rinse and repeat
model_probit_vote15_imp <- glm(vote15_imp ~ sons + dtrs + ageco + agecosq +
                              revoffco + nslave + dist2 + vsecr + vbank +
                              ddebt + pols + lawyer,
                              data=dataset_missing, 
                              family=binomial(link="probit"))

pred_vote15_imp <- ggpredict(model_probit_vote15_imp, c("sons"))
colnames(pred_vote15_imp) <- c("x.imp", "predicted.imp", "conf.low.imp",
                              "conf.high.imp", "group.imp")

pred_vote15 <- ggpredict(model_probit8_R, c("sons"))

p15 <- cbind(pred_vote15_imp, pred_vote15)

pred_sons_vote15 <- ggplot(p15, aes()) +
  geom_line(aes(x=x,y=predicted, color="Historical supplement"), size=1) +
  #geom_ribbon(aes(x=x, y=predicted, ymin = conf.low, ymax = conf.high), alpha = .25) +
  geom_line(aes(x=x.imp, y=predicted.imp, color="Amelia imputed"), size=1) +
  #geom_ribbon(aes(x=x.imp, y=predicted.imp, ymin = conf.low.imp, ymax = conf.high.imp), alpha = .25) +
  xlab("Number of sons") +
  ylab("Probability of pro-national vote") +
  ylim(c(0,1)) +
  xlim(c(0,3)) +
  ggtitle("Military Responsibility") +
  labs(color="")  +
  scale_color_manual(values = c("darkred", "darkblue")) +
  theme_bw() +
  theme(legend.position = "none",
        plot.title = element_text(size=12))

grid.arrange(pred_sons_vote2, pred_sons_vote9, pred_sons_vote15, nrow=1,
             top="Figure 11: Individual Vote Predicted Probability for Imputed Data")
```

```{r, fig.show='hold', fig.align='center', out.width="80%", out.height="80%"}
knitr::include_graphics("fig11.png")
```

### Missingness

As Figure 10 shows, the original vote data employed by Pope and Schmidt (2021) is rife with missingness--more than half of all votes are missing. This missingness was originally remedied by McDonald (1958) who used historical sources to supplement the vote data, providing estimates of how delegates *would* have voted. In this section, I compare this historical estimation method to data imputation using `R`'s `Amelia` package. Because such a large percentage of votes are missing from the original data, imputation was possible only on three votes which had enough variance and lacked multicollinearity issues: National Veto, State Credit, and Military Responsibility. The preferred index could thus not be calculated using only the original data; just these three individual votes.

Predicted value plots of this imputation for each of the three votes are compared to the historically supplemented data used by Pope and Schmidt (2021) in Figure 11. While the historically supplemented data consistently predicts that delegates with more sons will have a higher probability of voting in favor of national government expansion, the imputed data shows mixed results. For the National Veto vote the imputed data results in the opposite; more sons lowers the probability of voting to expand the government. For the State Credit vote the results are similar to that of the historical data, albeit less drastic. For Military Responsibility there appears to be no change in vote probability using imputed data. These results indicate, rather unsurprisingly, that addressing missingness through informed historical supplementation is key to seeing an effect for these three votes. Were this supplemental data not available, imputed data may have obfuscated the results of the study.

## References
\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}
\setlength{\parskip}{8pt}
\noindent

Dougherty, K., & Heckelman, J. C. (2009). Delegate Votes on 28 Motions at the United States Constitutional Convention, 1787: Version 1 [Data set]. ICPSR Data Holdings. Inter-university Consortium for Political and Social Research (ICPSR).

Glenn, T. A. (2013). *William Churchill Houston 1746-1788*. Book on Demand.

Holt, W. W. (n.d.). Wythe, George (1726 or 1727â€“1806). Retrieved June 4, 2021, from EncyclopediaVirginia.org website: https://encyclopediavirginia.org/entries/wythe-george-1726-or-1727-1806/

McDonald, F. (1958). *We, the People: The Economic Origins of the Constitution*. Chicago, IL: University of Chicago Press.

Pope, J. C., & Schmidt, S. J. (2021). Father Founders: Did Child Gender Affect Voting at the Constitutional Convention? *American Journal of Political Science*, (ajps.12573). doi:10.1111/ajps.12573

The Founding Fathers: New Jersey. (2015, November 6). Retrieved June 4, 2021, from Archives.gov website: https://www.archives.gov/founding-docs/founding-fathers-new-jersey

## Appendix A

```{r, fig.show='hold'}
var_names <- c('index1',
                'vote2',
                'anti5',
                'anti6',
                'anti7',
                'vote8',
                'vote9',
                'anti14',
                'vote15',
                'sons',
                'dtrs',
                'ageco',
                'agecosq',
                'revoffco',
                'nslave',
                'dist2',
                'vsecr',
                'vbank',
                'ddebt',
                'pols',
                'lawyer')

full_names <- c("Preferred Index",
                "National Veto",
                "Debtor Legislators",
                "Cong. Quorum.",
                "National Exports",
                "Militia Control",
                "State Credit",
                "Navigation Acts",
                "Military Responsibility",
                "Number of sons",
                "Number of daughters",
                "Age",
                "Age squared",
                "Revolutionary war officer",
                "Logged number of slaves",
                "Distance to navigable coastline",
                "Public securities (1000s, 1787 dollars)",
                "Private securities (1000s, 1787 dollars)",
                "Debtor (dummy)",
                "Politician",
                "Lawyer")                

kable(cbind(var_names, full_names), col.names=c("Variable name","Full name"),
      caption="Variable Key", booktabs=T) %>%
  kable_styling(latex_options = "hold_position")
      
```



